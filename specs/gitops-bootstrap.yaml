# GitOps Bootstrap Specification
---
# INITIATIVE METADATA
initiative:
  name: "gitops-bootstrap"
  started: "2026-01-04"
  status: "phase-5-ready"
  repository: "mothership-gitops"
  depends_on: "omni-deployment (production-cluster-operational)"
  conversation_export: "gitops-boot-convo.txt"
  summary: |
    GitOps foundation for talos-prod-01 cluster.
    Single kubectl apply bootstrap, everything else via ArgoCD.
    Components: ArgoCD (HA), Tailscale Operator, Longhorn, External Secrets, Netdata.
    Phases 1-4 complete. Phase 5 (validation/docs) ready for new session.

---
# DEFINITION OF DONE
definition_of_done:
  - "Single bootstrap command deploys entire stack"
  - "All components managed via ArgoCD Applications"
  - "Secrets sourced from Infisical via ESO"
  - "Services accessible via Tailscale (no public exposure)"
  - "Persistent storage available via Longhorn"
  - "Cluster monitoring operational via Netdata"
  - "ArgoCD running in HA mode with persistent storage"
  - "Recovery procedure documented and tested"

---
# ARCHITECTURE DECISIONS

decisions:
  repository_structure:
    decision: "Separate GitOps repo (mothership-gitops)"
    rationale:
      - "Omni-Scale = how cluster exists (infra provisioning)"
      - "mothership-gitops = what runs on cluster (workloads)"
      - "Different change cadences, clean git history"
      - "Enables multi-cluster GitOps future"
    change_cost: "low - repo is new"

  bootstrap_pattern:
    decision: "App of Apps with sync waves"
    rationale:
      - "Single entry point (bootstrap.yaml)"
      - "Ordered deployment via sync waves"
      - "Self-managing ArgoCD upgrades"
    reference: "https://argo-cd.readthedocs.io/en/stable/operator-manual/cluster-bootstrapping/"

  secrets_bootstrap:
    decision: "One manual secret for Infisical auth, everything else via ESO"
    rationale:
      - "Breaks chicken-and-egg cleanly"
      - "Single manual step, documented"
      - "All other secrets GitOps-managed"
    manual_secret:
      name: "universal-auth-credentials"
      namespace: "external-secrets"
      keys: ["clientId", "clientSecret"]
      source: "Infisical Universal Auth (Machine Identity)"

  tailscale_operator_mode:
    decision: "Standard operator (expose services)"
    rationale: "Cluster access already works via Omni K8s proxy"
    optional_future: "API server proxy for Omni-independent kubectl access"
    reference: "https://tailscale.com/kb/1236/kubernetes-operator"

  longhorn_replicas:
    decision: "2 replicas (Golf + Hotel workers only)"
    rationale:
      - "Research doc discourages workloads on control planes"
      - "2 workers = 2 replicas max for full redundancy"
      - "Can increase if workers added"
    storage_class_default: true

  argocd_ha_strategy:
    decision: "Bootstrap non-HA, self-upgrade to HA after Longhorn"
    rationale:
      - "Non-HA ArgoCD needs no PVCs"
      - "HA requires Redis cluster + repo-server PVCs"
      - "Self-upgrade via separate Application with sync-wave 99"
    risk_window: "Hours (acceptable for homelab)"

---
# REPOSITORY STRUCTURE

repo_structure:
  root: "mothership-gitops"
  description: |
    bootstrap/ - kubectl apply entry point (bootstrap.yaml, namespace.yaml)
    apps/ - ArgoCD Applications
      root.yaml - App of Apps
      argocd/ - ArgoCD manifests (non-HA + HA upgrade)
      external-secrets/ - ESO + ClusterSecretStore
      tailscale-operator/ - Tailscale operator
      longhorn/ - Storage
      netdata/ - Monitoring
    workloads/ - Future application deployments

---
# SYNC WAVE ORDER

sync_waves:
  wave_1:
    component: "ArgoCD (non-HA)"
    method: "bootstrap.yaml includes directly"
    requires: "nothing"
    provides: "GitOps engine"

  wave_2:
    component: "External Secrets Operator"
    method: "ArgoCD Application"
    requires: "ArgoCD"
    provides: "CRDs, controller (no auth yet)"

  wave_3:
    component: "ClusterSecretStore"
    method: "ArgoCD Application (same as ESO)"
    requires: "ESO CRDs + manual infisical-auth secret"
    provides: "Infisical connectivity"

  wave_4:
    component: "Tailscale Operator"
    method: "ArgoCD Application"
    requires: "ESO (for OAuth client secret)"
    provides: "Tailscale service exposure"
    secrets_from_infisical:
      - "tailscale-operator-oauth-client"

  wave_5:
    component: "Longhorn"
    method: "ArgoCD Application"
    requires: "nothing (optional: ESO for encryption)"
    provides: "PersistentVolumeClaims"
    config:
      default_replicas: 2
      default_storage_class: true

  wave_6:
    component: "Netdata"
    method: "ArgoCD Application"
    requires: "ESO (for claim token)"
    provides: "Cluster monitoring"
    secrets_from_infisical:
      - "netdata-claim-token"

  wave_99:
    component: "ArgoCD HA Upgrade"
    method: "ArgoCD Application (self-managed)"
    requires: "Longhorn PVCs"
    provides: "HA ArgoCD (Redis cluster, persistent repo-server)"
    sync_policy: "manual (safety)"

---
# BOOTSTRAP PROCEDURE

bootstrap:
  prerequisites:
    - "talos-prod-01 cluster operational"
    - "kubectl access via Omni proxy"
    - "Infisical Universal Auth credentials available"
    - "Tailscale OAuth client created (tag: k8s-operator)"
    - "Netdata claim token from cloud.netdata.cloud"

  steps:
    step_1:
      name: "Create Infisical auth secret"
      type: "manual"
      command: |
        kubectl create namespace external-secrets
        kubectl create secret generic universal-auth-credentials \
          --from-literal=clientId=<INFISICAL_CLIENT_ID> \
          --from-literal=clientSecret=<INFISICAL_CLIENT_SECRET> \
          -n external-secrets

    step_2:
      name: "Bootstrap GitOps"
      type: "automated"
      command: |
        kubectl apply -f https://raw.githubusercontent.com/<user>/mothership-gitops/main/bootstrap/bootstrap.yaml
      note: "Or clone repo first: kubectl apply -f bootstrap/bootstrap.yaml"

    step_3:
      name: "Monitor deployment"
      type: "verification"
      commands:
        - "kubectl get applications -n argocd"
        - "kubectl get pods -n argocd"
        - "kubectl get pods -n external-secrets"
        - "kubectl get pods -n tailscale-operator"
        - "kubectl get pods -n longhorn-system"
        - "kubectl get pods -n netdata"

    step_4:
      name: "Trigger ArgoCD HA upgrade"
      type: "manual"
      timing: "After Longhorn healthy"
      command: |
        argocd app sync argocd-ha
      note: "Or via ArgoCD UI"

  recovery:
    description: "Full cluster rebuild from Git"
    steps:
      - "Rebuild cluster via Omni (specs/omni.yaml)"
      - "Run bootstrap step 1 (manual secret)"
      - "Run bootstrap step 2 (kubectl apply)"
      - "Wait for sync"
    note: "Workload data in Longhorn lost unless backed up externally"

---
# COMPONENT CONFIGURATIONS

components:
  argocd:
    chart: "argo/argo-cd"
    version: "~7.x" # Renovate managed
    namespace: "argocd"
    base_config:
      server:
        extraArgs:
          - "--insecure" # TLS via Tailscale
      configs:
        params:
          server.insecure: true
    ha_overlay:
      redis-ha:
        enabled: true
      controller:
        replicas: 2
      server:
        replicas: 2
        pdb:
          enabled: true
      repoServer:
        replicas: 2
        pdb:
          enabled: true
        persistence:
          enabled: true
          storageClass: "longhorn"

  external_secrets:
    chart: "external-secrets/external-secrets"
    version: "~0.x"
    namespace: "external-secrets"
    config:
      installCRDs: true
    cluster_secret_store:
      name: "infisical"
      provider: "infisical"
      auth:
        universalAuthCredentials:
          clientId:
            name: "universal-auth-credentials"
            namespace: "external-secrets"
            key: "clientId"
          clientSecret:
            name: "universal-auth-credentials"
            namespace: "external-secrets"
            key: "clientSecret"
      secretsScope:
        projectSlug: "mothership-s0-ew"
        environmentSlug: "prod"
      # hostAPI: https://app.infisical.com (default for cloud)

  tailscale_operator:
    chart: "tailscale/tailscale-operator"
    version: "~1.x"
    namespace: "tailscale-operator"
    config:
      oauth:
        clientId: "" # From ESO
        clientSecret: "" # From ESO
      operatorConfig:
        hostname: "talos-prod-operator"
        tags:
          - "tag:k8s-operator"
    secrets_required:
      - name: "tailscale-operator-oauth"
        infisical_path: "/tailscale-operator"
        mapping:
          CLIENT_ID: "clientId" # Infisical → K8s secret key
          CLIENT_SECRET: "clientSecret"

  longhorn:
    chart: "longhorn/longhorn"
    version: "~1.x"
    namespace: "longhorn-system"
    config:
      defaultSettings:
        defaultReplicaCount: 2
        storageMinimalAvailablePercentage: 10
        defaultDataPath: "/var/lib/longhorn"
      persistence:
        defaultClass: true
        defaultClassReplicaCount: 2

  netdata:
    chart: "netdata/netdata"
    version: "~3.x"
    namespace: "netdata"
    config:
      image:
        tag: "stable"
      parent:
        envFrom:
          - secretRef:
              name: netdata-claiming
      child:
        envFrom:
          - secretRef:
              name: netdata-claiming
    secrets_required:
      - name: "netdata-claiming"
        infisical_path: "/netdata"
        keys:
          NETDATA_CLAIM_TOKEN: "claiming token from cloud.netdata.cloud"
          NETDATA_CLAIM_ROOMS: "room ID(s) comma-separated"

---
# INFISICAL SECRETS STRUCTURE

infisical:
  projectSlug: "mothership-s0-ew"
  environmentSlug: "prod"
  paths:
    /tailscale-operator:
      CLIENT_ID: "tskey-client-..." # Maps to clientId in K8s secret
      CLIENT_SECRET: "..." # Maps to clientSecret in K8s secret
    /netdata:
      NETDATA_CLAIM_TOKEN: "..." # Claim token from cloud.netdata.cloud
      NETDATA_CLAIM_ROOMS: "..." # Room ID(s), comma-separated
    /argocd:
      # Future: ADMIN_PASSWORD, OIDC_SECRET
    /longhorn:
      # Future: BACKUP_ACCESS_KEY, BACKUP_SECRET_KEY

---
# CONSTRAINTS

constraints:
  worker_only_storage:
    description: "Longhorn runs on workers only (Golf, Hotel)"
    rationale: "Research doc discourages control plane workloads"
    impact: "Max 2 replicas until workers added"

  single_manual_secret:
    description: "One kubectl create secret for Infisical auth"
    rationale: "Breaks ESO chicken-and-egg"
    recovery_impact: "Must recreate manually after cluster rebuild"

  argocd_ha_manual_sync:
    description: "HA upgrade requires manual trigger"
    rationale: "Safety gate - ensure Longhorn healthy first"
    pattern: "sync-wave 99 + manual sync policy"

  tailscale_api_proxy_deferred:
    description: "API server proxy not implemented initially"
    rationale: "Omni K8s proxy provides kubectl access"
    future: "Add if Omni-independent access needed"

  talos_longhorn_disk_config:
    description: "Longhorn disks must be manually configured on Talos nodes"
    rationale: "Auto-discovery doesn't work despite mount patches in cluster template"
    resolution: "Patch nodes.longhorn.io with default-disk config after install"
    command: |
      kubectl patch nodes.longhorn.io <node> -n longhorn-system --type=merge -p \
        '{"spec":{"disks":{"default-disk":{"allowScheduling":true,"path":"/var/lib/longhorn","storageReserved":0}}}}'

  pod_security_privileged:
    description: "Longhorn and Netdata namespaces require privileged PSA"
    rationale: "Both run privileged containers (storage/monitoring)"
    resolution: "Add label pod-security.kubernetes.io/enforce=privileged to namespaces"

  infisical_path_scoped_stores:
    description: "Separate ClusterSecretStores per Infisical path"
    rationale: "Single store with secretsPath only works for one path"
    resolution: "Create infisical-tailscale, infisical-netdata stores with path-specific secretsScope"

  netdata_claiming_via_env:
    description: "Netdata claiming requires environment variables, not inline Helm values"
    rationale: "Chart lacks existingSecret option for claiming credentials"
    resolution: "Use envFrom.secretRef to inject NETDATA_CLAIM_TOKEN and NETDATA_CLAIM_ROOMS from ESO-managed secret"

  redis_ha_topology_mismatch:
    description: "Redis HA 3rd replica stuck Pending - not resource constraints, topology mismatch"
    rationale: |
      Redis HA StatefulSet requires 3 replicas with pod anti-affinity (one per node).
      Cluster has 5 nodes: 3 control-plane (tainted NoSchedule), 2 workers.
      Only 2 schedulable nodes exist, so 3rd replica can never schedule.
    symptom: "argocd-redis-ha-server-2 and haproxy replica Pending for 41+ hours"
    resolution: "Add 3rd worker node via Omni. Pods will auto-schedule once node joins."
    alternative: "Scale redis-ha replicas to 2 (accepts reduced HA) or add control-plane tolerations"
    status: "resolved"
    discovered: "2026-01-06"
    resolved: "2026-01-08"
    notes: |
      3rd worker (Foxtrot) added via matrix-worker-foxtrot MachineClass.
      Node talos-udo-ld3 joined, Redis HA 3/3, all pods Running.

  node_role_label_blocked:
    description: "node-role.kubernetes.io/* labels cannot be set by kubelet"
    rationale: |
      Kubernetes NodeRestriction admission controller blocks kubelets from setting
      labels in protected namespaces: node-role.kubernetes.io/*, kubernetes.io/*
      (except specific allowed keys like topology.kubernetes.io/zone).
    symptom: |
      Node stuck in reboot loop with error:
      "nodes X is forbidden: is not allowed to modify labels: node-role.kubernetes.io/worker"
    resolution: |
      Remove node-role.kubernetes.io/worker from machine.nodeLabels in cluster template.
      Either skip the label entirely, use a custom namespace (e.g., omni.sidero.dev/role),
      or apply labels externally via kubectl after node joins.
    status: "resolved"
    discovered: "2026-01-06"
    resolved: "2026-01-06"
    files_updated:
      - "clusters/talos-prod-01.yaml (commented out blocked labels for all workers)"

  workload_resource_requests_missing:
    description: "Most pods have no CPU/memory requests set"
    rationale: |
      ArgoCD Helm chart and other workloads deployed with default values - no resource requests.
      Scheduler cannot make informed placement decisions. Current allocation shows:
        - Worker nodes: 13% CPU requested, 0% memory requested
        - Longhorn instance-manager is only pod with meaningful requests (954m CPU)
    impact:
      - "Scheduler flies blind - can't predict capacity exhaustion"
      - "No eviction priority during resource pressure"
      - "Cluster appears empty when it's not"
    resolution: |
      Add resource requests/limits to Helm values for:
        - ArgoCD components (server, controller, repo-server, redis)
        - External Secrets controller
        - Tailscale operator
        - Netdata parent/child
      Use observed usage as baseline, add headroom.
    status: "backlog"
    discovered: "2026-01-06"

---
# PHASES

phases:
  phase_1:
    name: "Repository Setup"
    status: "complete"
    completed: "2026-01-04"
    tasks:
      - "Create mothership-gitops repository ✓"
      - "Initialize directory structure ✓"
      - "Create bootstrap.yaml ✓"
      - "Create App of Apps root ✓"
      - "Configure Renovate for Helm chart updates"
    exit_criteria:
      - "Repo structure matches spec ✓"
      - "bootstrap.yaml applies cleanly to empty cluster ✓"
    artifacts:
      - "https://github.com/basher83/mothership-gitops"

  phase_2:
    name: "Core Infrastructure Apps"
    status: "complete"
    completed: "2026-01-04"
    depends_on: ["phase_1"]
    tasks:
      - "ArgoCD Application (non-HA) ✓"
      - "External Secrets Application + ClusterSecretStore ✓"
      - "Infisical project 'mothership-s0-ew' configured ✓"
      - "Populate Infisical secrets ✓"
    exit_criteria:
      - "ArgoCD UI accessible ✓"
      - "ESO controller running ✓"
      - "ClusterSecretStore shows Ready ✓"
    lessons_learned:
      - "ESO API version is v1, not v1beta1"
      - "Machine Identity needs explicit project access in Infisical"

  phase_3:
    name: "Platform Services"
    status: "complete"
    completed: "2026-01-04"
    depends_on: ["phase_2"]
    tasks:
      - "Tailscale Operator Application ✓"
      - "Longhorn Application ✓"
      - "Netdata Application ✓"
    exit_criteria:
      - "Tailscale operator running, connected to tailnet ✓"
      - "Longhorn UI accessible ✓"
      - "StorageClass 'longhorn' available ✓"
      - "Netdata child pods on all nodes ✓"
      - "Netdata visible in cloud.netdata.cloud"
    lessons_learned:
      - "Longhorn pre-upgrade hook fails on fresh install - delete job to unblock"
      - "Talos nodes need manual Longhorn disk config"
      - "Privileged PSA required for longhorn-system, netdata, and tailscale-operator namespaces"
      - "Separate ClusterSecretStores needed per Infisical path"
      - "Netdata claiming via envFrom.secretRef, not inline values (chart lacks existingSecret)"
      - "ESO adds default fields to ExternalSecrets - use ignoreDifferences in parent apps"
      - "ESO-managed Secrets cause Helm app drift - ignore /data, /metadata/annotations, /metadata/labels"
      - "Ingress without controller causes perpetual Progressing health - disable unused ingress"
      - "Tailscale proxy pods require privileged mode - namespace needs PodSecurity exemption"

  phase_4:
    name: "ArgoCD HA Upgrade"
    status: "complete"
    completed: "2026-01-04"
    depends_on: ["phase_3"]
    tasks:
      - "Create ArgoCD HA Application ✓"
      - "Verify Longhorn PVC provisioning ✓"
      - "Trigger sync ✓"
      - "Verify HA components healthy ✓"
    exit_criteria:
      - "Redis HA running ✓ (2/3 - sufficient for quorum)"
      - "Multiple ArgoCD replicas ✓ (2x controller, server, repo-server)"
      - "PVCs bound to Longhorn volumes ✓"
    notes:
      - "3rd Redis replica pending - topology mismatch, not resource constraints (see constraint: redis_ha_topology_mismatch)"
      - "Root cause: 3 replicas with anti-affinity, only 2 worker nodes (control-plane tainted NoSchedule)"
      - "Resolution: Add 3rd worker via Omni - pods will auto-schedule"
      - "Helm shows 'failed' due to CRD conflicts but deployment succeeded"

  phase_5:
    name: "Validation & Documentation"
    status: "ready"
    depends_on: ["phase_4"]
    tasks:
      - "Test recovery procedure"
      - "Document bootstrap in repo README"
      - "Update Omni-Scale STATE.md with GitOps status"
      - "Update kernel.md if needed"
    exit_criteria:
      - "Recovery procedure tested (destroy cluster, rebuild, verify)"
      - "README documents bootstrap steps and architecture"
      - "Cross-references updated"
    handoff_context:
      conversation_export: "gitops-boot-convo.txt"
      spec_file: "specs/gitops-bootstrap.yaml"
      repo: "https://github.com/basher83/mothership-gitops"
    recovery_test_steps:
      - "Document current state (kubectl get all -A > before.txt)"
      - "Delete cluster via Omni UI or omnictl"
      - "Recreate cluster from Omni-Scale/clusters/talos-prod-01.yaml"
      - "Create manual secret (universal-auth-credentials)"
      - "kubectl apply -f bootstrap/bootstrap.yaml"
      - "Wait for sync, patch Longhorn nodes"
      - "Verify services match before.txt"

---
# REFERENCES

references:
  argocd_docs: "https://argo-cd.readthedocs.io/"
  external_secrets_docs: "https://external-secrets.io/"
  external_secrets_infisical: "https://external-secrets.io/latest/provider/infisical/"
  tailscale_operator: "https://tailscale.com/kb/1236/kubernetes-operator"
  tailscale_api_proxy: "https://tailscale.com/kb/1437/kubernetes-operator-api-server-proxy"
  longhorn_docs: "https://longhorn.io/docs/"
  netdata_helm: "https://learn.netdata.cloud/docs/netdata-agent/installation/kubernetes"
  app_of_apps: "https://argo-cd.readthedocs.io/en/stable/operator-manual/cluster-bootstrapping/"
  research_doc: "docs/guides/talos-proxmox-research.md"
  omni_spec: "specs/omni.yaml"
